import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
import sys
import openai  # æ·»åŠ æ­¤å¯¼å…¥ä»¥ä¿®å¤ NameError: openai æœªå®šä¹‰

# SQLite3 ç‰ˆæœ¬ä¿®å¤
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass  # å¦‚æœ pysqlite3 ä¸å¯ç”¨ï¼Œç»§ç»­ä½¿ç”¨ç³»ç»Ÿ sqlite3

# ChromaDB å¯¼å…¥å’Œé”™è¯¯å¤„ç†
try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError as e:
    st.error(f"ChromaDBå¯¼å…¥å¤±è´¥: {e}")
    CHROMA_AVAILABLE = False
except RuntimeError as e:
    if "sqlite3" in str(e).lower():
        st.error("SQLite3 ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œæ­£åœ¨ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
        CHROMA_AVAILABLE = False
    else:
        st.error(f"ChromaDBè¿è¡Œæ—¶é”™è¯¯: {e}")
        CHROMA_AVAILABLE = False

def get_retriever():
    if not CHROMA_AVAILABLE:
        st.warning("ChromaDBä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æœç´¢ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
        return None
    
    try:
        # è·å– API å¯†é’¥
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("æœªæ‰¾åˆ° OpenAI API å¯†é’¥ï¼Œè¯·åœ¨ Streamlit secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENAI_API_KEY")
            return None
        
        # å®šä¹‰ Embeddings
        embedding = OpenAIEmbeddings(
            openai_api_key=api_key
        )
        # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
        persist_directory = './data_base/vector_db/chroma'
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(persist_directory):
            st.warning(f"å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {persist_directory}")
            return None
        
        # åŠ è½½æ•°æ®åº“
        vectordb = Chroma(
            persist_directory=persist_directory,
            embedding_function=embedding
        )
        return vectordb.as_retriever()
    
    except Exception as e:
        st.error(f"åˆ›å»ºæ£€ç´¢å™¨æ—¶å‡ºé”™: {e}")
        return None

def simple_fallback_retriever(query, knowledge_base=None):
    """
    ç®€å•çš„å¤‡ç”¨æ£€ç´¢å™¨ï¼Œå½“ChromaDBä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    if knowledge_base is None:
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€äº›é¢„è®¾çš„çŸ¥è¯†åº“å†…å®¹
        knowledge_base = [
            "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹çŸ¥è¯†åº“å†…å®¹ã€‚",
            "å½“ChromaDBä¸å¯ç”¨æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªç®€å•çš„æ–‡æœ¬åŒ¹é…ã€‚",
            "æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„çŸ¥è¯†åº“å†…å®¹ã€‚"
        ]
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    relevant_docs = []
    for doc in knowledge_base:
        if any(keyword.lower() in doc.lower() for keyword in query.split()):
            relevant_docs.append(doc)
    
    return relevant_docs[:3]  # è¿”å›æœ€å¤š3ä¸ªç›¸å…³æ–‡æ¡£

def combine_docs(docs):
    if isinstance(docs, dict) and "context" in docs:
        return "\n\n".join(doc.page_content for doc in docs["context"])
    elif isinstance(docs, list):
        return "\n\n".join(docs)
    return str(docs) if docs else ""

def get_qa_chain_without_retriever():
    """
    ä¸ä½¿ç”¨æ£€ç´¢å™¨çš„ç®€å•é—®ç­”é“¾
    """
    try:
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("æœªæ‰¾åˆ° OpenAI API å¯†é’¥")
            return None
        
        llm = ChatOpenAI(
            model_name="gpt-4o", 
            temperature=0,
            openai_api_key=api_key
        )
        
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰å¸®åŠ©çš„å›ç­”ã€‚"
            "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"
        )
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])
        
        return qa_prompt | llm | StrOutputParser()
    
    except Exception as e:
        st.error(f"åˆ›å»ºé—®ç­”é“¾æ—¶å‡ºé”™: {e}")
        return None

def get_qa_history_chain():
    retriever = get_retriever()
    
    try:
        api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("æœªæ‰¾åˆ° OpenAI API å¯†é’¥ï¼Œè¯·åœ¨ Streamlit secrets ä¸­è®¾ç½® OPENAI_API_KEY")
            return None
        
        llm = ChatOpenAI(
            model_name="gpt-4o", 
            temperature=0,
            openai_api_key=api_key
        )
        
        if retriever is None:
            # ä½¿ç”¨ç®€å•çš„é—®ç­”é“¾ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            st.info("ä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼Œæ— æ£€ç´¢åŠŸèƒ½")
            return get_qa_chain_without_retriever()
        
        condense_question_system_template = (
            "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ"
            "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
        )
        
        condense_question_prompt = ChatPromptTemplate([
            ("system", condense_question_system_template),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])
        
        retrieve_docs = RunnableBranch(
            (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever,),
            condense_question_prompt | llm | StrOutputParser() | retriever,
        )
        
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
            "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
            "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
            "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
            "\n\n"
            "{context}"
        )
        
        qa_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ])
        
        qa_chain = (
            RunnablePassthrough().assign(context=combine_docs)
            | qa_prompt
            | llm
            | StrOutputParser()
        )
        
        qa_history_chain = RunnablePassthrough().assign(
            context=retrieve_docs,
        ).assign(answer=qa_chain)
        
        return qa_history_chain
    
    except Exception as e:
        st.error(f"åˆ›å»ºé—®ç­”é“¾æ—¶å‡ºé”™: {e}")
        return get_qa_chain_without_retriever()

def gen_response(chain, input_text, chat_history):
    if not chain:
        yield "æŠ±æ­‰ï¼Œç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å›ç­”é—®é¢˜ã€‚"
        return
    
    try:
        response = chain.stream({
            "input": input_text,
            "chat_history": chat_history
        })
        for res in response:
            if isinstance(res, dict) and "answer" in res:
                yield res["answer"]
            elif isinstance(res, str):
                yield res
    except openai.AuthenticationError as e:
        yield f"è®¤è¯é”™è¯¯: æä¾›çš„ OpenAI API å¯†é’¥æ— æ•ˆã€‚è¯·è®¿é—® https://platform.openai.com/account/api-keys è·å–æ–°å¯†é’¥ï¼Œå¹¶æ›´æ–° Streamlit secrets æˆ–ç¯å¢ƒå˜é‡ã€‚"
    except Exception as e:
        yield f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"

def main():
    st.set_page_config(
        page_title="é—®ç­”åŠ©æ‰‹",
        page_icon="ğŸ¦œ",
        layout="wide"
    )
    
    st.markdown('### ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')
    
    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    if not CHROMA_AVAILABLE:
        st.warning("âš ï¸ ChromaDBä¸å¯ç”¨ï¼Œæ­£åœ¨ä½¿ç”¨ç®€åŒ–æ¨¡å¼")
    else:
        st.success("âœ… ç³»ç»Ÿæ­£å¸¸è¿è¡Œ")
    
    # å­˜å‚¨å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾
    if "qa_history_chain" not in st.session_state:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ..."):
            st.session_state.qa_history_chain = get_qa_history_chain()
    
    # å»ºç«‹å®¹å™¨
    messages = st.container(height=550)
    
    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
    for message in st.session_state.messages:
        with messages.chat_message(message[0]):
            st.write(message[1])
    
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append(("human", prompt))
        
        # æ˜¾ç¤ºå½“å‰ç”¨æˆ·è¾“å…¥
        with messages.chat_message("human"):
            st.write(prompt)
        
        # ç”Ÿæˆå›å¤
        answer_generator = gen_response(
            chain=st.session_state.qa_history_chain,
            input_text=prompt,
            chat_history=st.session_state.messages
        )
        
        # æµå¼è¾“å‡º
        with messages.chat_message("ai"):
            output = st.write_stream(answer_generator)
        
        # å°†è¾“å‡ºå­˜å…¥å¯¹è¯å†å²
        st.session_state.messages.append(("ai", output))

if __name__ == "__main__":
    main()
