import io, os, sys Â # å¯¼å…¥æ ‡å‡†åº“ï¼Œç”¨äºå­—èŠ‚æµå¤„ç†ã€æ“ä½œç³»ç»Ÿæ¥å£å’Œç³»ç»Ÿé€€å‡º
from dotenv import load_dotenv Â # ä» python-dotenv åŠ è½½ .env ç¯å¢ƒå˜é‡

import streamlit as st Â # å¯¼å…¥ Streamlitï¼Œç”¨äºæ­å»ºç½‘é¡µåº”ç”¨
from langchain_openai import ChatOpenAI, OpenAIEmbeddings Â # å¯¼å…¥ LangChain çš„ OpenAI LLM æ¥å£å’ŒåµŒå…¥æ¨¡å‹
from langchain_chroma import Chroma Â # æ”¹ç”¨æ–°ç‰ˆ langchain-chroma
from langchain_core.output_parsers import StrOutputParser Â # å¯¼å…¥è¾“å‡ºè§£æå™¨
from langchain_core.prompts import ChatPromptTemplate Â # å¯¼å…¥å¯¹è¯æç¤ºæ¨¡æ¿
from langchain_core.runnables import RunnableBranch, RunnablePassthrough Â # å¯¼å…¥å¯è¿è¡Œç»„ä»¶

import PyPDF2 Â # å¯¼å…¥ PyPDF2 ç”¨äºè§£æ PDF æ–‡æœ¬
import docx Â # å¯¼å…¥ python-docx ç”¨äºè§£æ .docx æ–‡æ¡£

# â€”â€” ç¯å¢ƒ & API Key â€”â€” #
load_dotenv() Â # è¯»å–é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "") Â # ä»ç¯å¢ƒå˜é‡ä¸­è·å– OpenAI API Key
if not OPENAI_API_KEY:
Â  Â  st.error("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY") Â # å¦‚æœæœªè®¾ç½®ï¼Œæ˜¾ç¤ºé”™è¯¯æç¤º
Â  Â  sys.exit(1) Â # é€€å‡ºç¨‹åº

# â€”â€” é¡µé¢ & æ ·å¼ â€”â€” #
st.set_page_config(page_title="é‡åº†ç§‘æŠ€å¤§å­¦é—®ç­”ç³»ç»Ÿ", layout="wide") Â # é…ç½® Streamlit é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.markdown(
Â  Â  "<h1 style='text-align:center;color:#333;margin-bottom:16px;'>ğŸ“ é‡åº†ç§‘æŠ€å¤§å­¦é—®ç­”ç³»ç»Ÿ</h1>",
Â  Â  unsafe_allow_html=True, Â # å…è®¸ HTML æ¸²æŸ“ï¼Œç”¨äºè‡ªå®šä¹‰æ ‡é¢˜
)
st.markdown("""
<style>
.upload-box {
Â  border:1px dashed #bbb;
Â  border-radius:6px;
Â  height:200px;
Â  text-align:center;
Â  padding-top:60px;
Â  color:#666;
Â  margin-bottom:16px;
}
header, footer { visibility: hidden; } Â /* éšè—é»˜è®¤çš„ header å’Œ footer */
</style>
""", unsafe_allow_html=True) Â # æ³¨å…¥è‡ªå®šä¹‰ CSS æ ·å¼

# â€”â€” å‘é‡åº“ & RAG â€”â€” #
PERSIST_DIR = "workspaces/test_codespace/chroma_db" Â # å‘é‡åº“æŒä¹…åŒ–ç›®å½•
EMBEDDER = OpenAIEmbeddings(api_key=OPENAI_API_KEY, base_url="https://xiaoai.plus/v1")
# åˆå§‹åŒ– OpenAI åµŒå…¥æ¨¡å‹ï¼ŒæŒ‡å®š API Key å’Œæ¥å£åœ°å€

def split_text(text, chunk_size=500):
Â  Â  """æŒ‰å›ºå®šå¤§å°æ‹†åˆ†æ–‡æœ¬ä¸ºå¤šä¸ªç‰‡æ®µ"""
Â  Â  return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def extract_text(f):
Â  Â  """æ ¹æ®æ–‡ä»¶ç±»å‹æå–çº¯æ–‡æœ¬"""
Â  Â  name = f.name.lower()
Â  Â  data = f.read()
Â  Â  if name.endswith(".txt"):
Â  Â  Â  Â  return data.decode("utf-8", errors="ignore")
Â  Â  if name.endswith(".pdf"):
Â  Â  Â  Â  reader = PyPDF2.PdfReader(io.BytesIO(data))
Â  Â  Â  Â  return "\n".join(p.extract_text() or "" for p in reader.pages)
Â  Â  if name.endswith(".docx"):
Â  Â  Â  Â  d = docx.Document(io.BytesIO(data))
Â  Â  Â  Â  return "\n".join(p.text for p in d.paragraphs)
Â  Â  return ""

def index_file(f):
Â  Â  """å°†å•ä¸ªæ–‡ä»¶çš„æ–‡æœ¬æ‹†åˆ†ã€åµŒå…¥å¹¶å­˜å…¥ Chroma å‘é‡åº“"""
Â  Â  txt = extract_text(f)
Â  Â  if not txt:
Â  Â  Â  Â  return 0
Â  Â  chunks = split_text(txt)
Â  Â  db = Chroma(persist_directory=PERSIST_DIR, embedding_function=EMBEDDER)
Â  Â  db.add_texts(chunks)
Â  Â  # æ–°ç‰ˆ Chroma è‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨ db.persist()
Â  Â  return len(chunks)

def get_retriever():
Â  Â  """åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œç”¨äºåŸºäºæŸ¥è¯¢å‘é‡æ£€ç´¢æ–‡æ¡£"""
Â  Â  return Chroma(persist_directory=PERSIST_DIR, embedding_function=EMBEDDER).as_retriever()

def combine_docs(docs):
Â  Â  """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹æ‹¼æ¥ä¸ºå•ä¸ªä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
Â  Â  return "\n\n".join(d.page_content for d in docs["context"])

def build_chain():
Â  Â  """æ„å»ºä¸€ä¸ª RAG é—®ç­”é“¾ï¼ŒåŒ…æ‹¬å†å²æµ“ç¼©å’Œæœ€ç»ˆé—®ç­”"""
Â  Â  retriever = get_retriever()
Â  Â  llm = ChatOpenAI(model_name="gpt-4o", temperature=0,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â api_key=OPENAI_API_KEY, base_url="https://xiaoai.plus/v1")
Â  Â  # æ­¥éª¤ä¸€ï¼šå¦‚æœæœ‰å†å²åˆ™æµ“ç¼©ï¼Œå¦åˆ™ç›´æ¥ä½¿ç”¨è¾“å…¥
Â  Â  condense = ChatPromptTemplate([
Â  Â  Â  Â  ("system","æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼›å¦‚æ— å†å²åˆ™è¿”å›åŸé—®é¢˜ã€‚"),
Â  Â  Â  Â  ("placeholder","{chat_history}"),
Â  Â  Â  Â  ("human","{input}"),
Â  Â  ])
Â  Â  branch = RunnableBranch(
Â  Â  Â  Â  (lambda x: not x.get("chat_history"), (lambda x: x["input"]) | retriever),
Â  Â  Â  Â  condense | llm | StrOutputParser() | retriever,
Â  Â  )
Â  Â  # æ­¥éª¤äºŒï¼šåŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡åšæœ€ç»ˆé—®ç­”
Â  Â  prompt = ChatPromptTemplate.from_messages([
Â  Â  Â  Â  ("system",
Â  Â  Â  Â  Â "ä½ æ˜¯é‡åº†ç§‘æŠ€å¤§å­¦é—®ç­”ç³»ç»Ÿï¼Œä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”ï¼›"
Â  Â  Â  Â  Â "å¦‚æ— ç­”æ¡ˆï¼Œè¯·è¯´â€œæˆ‘ä¸çŸ¥é“â€ã€‚\n\n{context}"
Â  Â  Â  Â  ),
Â  Â  Â  Â  ("placeholder","{chat_history}"),
Â  Â  Â  Â  ("human","{input}"),
Â  Â  ])
Â  Â  qa = (RunnablePassthrough().assign(context=combine_docs)
Â  Â  Â  Â  Â  | prompt | llm | StrOutputParser())
Â  Â  return RunnablePassthrough().assign(context=branch).assign(answer=qa)

def get_answer(chain, q, hist):
Â  Â  """è°ƒç”¨ RAG é“¾å¹¶è¿”å› AI å›ç­”"""
Â  Â  res = chain.invoke({"input": q, "chat_history": hist})
Â  Â  return res["answer"]

# â€”â€” ä¸»é€»è¾‘ â€”â€” #
def main():
Â  Â  # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
Â  Â  if "msgs" not in st.session_state:
Â  Â  Â  Â  st.session_state.msgs = [] Â # å­˜å‚¨äººæœºå¯¹è¯çš„åˆ—è¡¨ [(role, msg), ...]
Â  Â  if "chain" not in st.session_state:
Â  Â  Â  Â  st.session_state.chain = build_chain()

Â  Â  col1, col2 = st.columns([3, 1]) Â # å·¦å³ä¸¤åˆ—å¸ƒå±€ï¼š3:1

Â  Â  # å³ä¾§ï¼šçŸ¥è¯†åº“ä¸Šä¼ ï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰
Â  Â  with col2:
Â  Â  Â  Â  st.markdown("### ğŸ“‚ ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“", unsafe_allow_html=True)
Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  "<div class='upload-box'>å°†æ–‡ä»¶æ‹–æ”¾æˆ–ç‚¹å‡»ä¸Šä¼ <br/>(txt/pdf/docxï¼Œæ”¯æŒå¤šé€‰)</div>",
Â  Â  Â  Â  Â  Â  unsafe_allow_html=True,
Â  Â  Â  Â  )
Â  Â  Â  Â  uploaded_files = st.file_uploader(
Â  Â  Â  Â  Â  Â  label="ä¸Šä¼ æ–‡ä»¶", Â # éç©º label
Â  Â  Â  Â  Â  Â  type=["txt","pdf","docx"],
Â  Â  Â  Â  Â  Â  accept_multiple_files=True,
Â  Â  Â  Â  Â  Â  key="uploader",
Â  Â  Â  Â  Â  Â  label_visibility="hidden" Â # éšè— label
Â  Â  Â  Â  )
Â  Â  Â  Â  if uploaded_files:
Â  Â  Â  Â  Â  Â  for f in uploaded_files:
Â  Â  Â  Â  Â  Â  Â  Â  cnt = index_file(f)
Â  Â  Â  Â  Â  Â  Â  Â  if cnt:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"{f.name} å·²ç´¢å¼• {cnt} æ®µæ–‡æœ¬")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{f.name} è§£æ/ç´¢å¼•å¤±è´¥")

Â  Â  # å·¦ä¾§ï¼šæ¶ˆæ¯åˆ—è¡¨å’Œç”¨æˆ·è¾“å…¥
Â  Â  with col1:
Â  Â  Â  Â  if prompt := st.chat_input("è¯´ç‚¹ä»€ä¹ˆâ€¦", key="chat_input"):
Â  Â  Â  Â  Â  Â  st.session_state.msgs.append(("human", prompt))
Â  Â  Â  Â  Â  Â  with st.spinner("AI æ­£åœ¨æ€è€ƒâ€¦"):
Â  Â  Â  Â  Â  Â  Â  Â  ans = get_answer(st.session_state.chain, prompt, st.session_state.msgs)
Â  Â  Â  Â  Â  Â  st.session_state.msgs.append(("ai", ans))
Â  Â  Â  Â  Â  Â  if len(st.session_state.msgs) > 20:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.msgs = st.session_state.msgs[-20:]

Â  Â  Â  Â  for role, msg in st.session_state.msgs:
Â  Â  Â  Â  Â  Â  with st.chat_message(role):
Â  Â  Â  Â  Â  Â  Â  Â  if role == "ai":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.code(msg)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(msg)

Â  Â  Â  Â  if st.session_state.msgs and st.session_state.msgs[-1][0] == "ai":
Â  Â  Â  Â  Â  Â  if st.button("ğŸ”„ é‡æ–°ç”Ÿæˆä¸Šæ¬¡å›ç­”"):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.msgs.pop()
Â  Â  Â  Â  Â  Â  Â  Â  last_user = next((m for r, m in reversed(st.session_state.msgs) if r == "human"), None)
Â  Â  Â  Â  Â  Â  Â  Â  if last_user:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("AI æ­£åœ¨é‡æ–°æ€è€ƒâ€¦"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_ans = get_answer(st.session_state.chain, last_user, st.session_state.msgs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.msgs.append(("ai", new_ans))

if __name__ == "__main__":
Â  Â  main() Â # æ‰§è¡Œä¸»é€»è¾‘

